#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯æ ¸å¿ƒåŠŸèƒ½ï¼ˆæ— éœ€çœŸå®PyTorchæ¨¡å‹ï¼‰
"""

import numpy as np
import pandas as pd
from pathlib import Path
import sys

print("=" * 70)
print("ğŸš€ èƒ¸éƒ¨Xå…‰åˆ†è¯Šç³»ç»Ÿ - å¿«é€Ÿæµ‹è¯•")
print("=" * 70)
print()

# æµ‹è¯•1: å¯¼å…¥æ£€æŸ¥
print("âœ… æµ‹è¯•1: æ ¸å¿ƒæ¨¡å—å¯¼å…¥")
print("-" * 70)
try:
    sys.path.insert(0, str(Path(__file__).parent))
    from src.validation.calibration import CalibrationMetrics, TemperatureScaling
    print("âœ“ CalibrationMetrics å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯•2: ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
print("\nâœ… æµ‹è¯•2: ç”Ÿæˆæ¨¡æ‹ŸéªŒè¯æ•°æ®")
print("-" * 70)

# æ¨¡æ‹Ÿå•æ ‡ç­¾å¤šåˆ†ç±»çš„éªŒè¯ç»“æœ
np.random.seed(42)
n_samples = 100
n_classes = 4

logits = np.random.randn(n_samples, n_classes)
targets = np.random.randint(0, n_classes, n_samples)
exp_logits = np.exp(logits - logits.max(axis=1, keepdims=True))
probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)

print(f"âœ“ ç”Ÿæˆ{n_samples}ä¸ªæ ·æœ¬çš„æ¨¡æ‹Ÿæ•°æ®")
print(f"  - Logits shape: {logits.shape}")
print(f"  - Targets shape: {targets.shape}")
print(f"  - æ¦‚ç‡èŒƒå›´: [{probs.min():.4f}, {probs.max():.4f}]")

# æµ‹è¯•3: ECEè®¡ç®—
print("\nâœ… æµ‹è¯•3: ECEï¼ˆæœŸæœ›æ ¡å‡†è¯¯å·®ï¼‰è®¡ç®—")
print("-" * 70)

try:
    metrics = CalibrationMetrics()
    ece = metrics.expected_calibration_error(probs, targets, n_bins=10)
    mce = metrics.maximum_calibration_error(probs, targets, n_bins=10)
    brier = metrics.brier_score(probs, targets)
    
    print(f"âœ“ ECEè®¡ç®—æˆåŠŸ: {ece:.4f}")
    print(f"âœ“ MCEè®¡ç®—æˆåŠŸ: {mce:.4f}")
    print(f"âœ“ Brier Score: {brier:.4f}")
    
    if ece <= 0.3:
        print("âœ“ ECE åœ¨åˆç†èŒƒå›´å†…")
    else:
        print(f"âš  ECEè¾ƒé«˜ï¼ˆ{ece:.4f}ï¼‰ï¼Œéœ€è¦æ ¡å‡†")
except Exception as e:
    print(f"âœ— ECEè®¡ç®—å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•4: Temperature Scaling
print("\nâœ… æµ‹è¯•4: Temperature Scaling æ ¡å‡†")
print("-" * 70)

try:
    calibrator = TemperatureScaling()
    calibrator.fit(logits, targets)
    calibrated_probs = calibrator.calibrate(logits)
    
    print(f"âœ“ Temperature Scaling æ‹ŸåˆæˆåŠŸ")
    print(f"  - æ¸©åº¦å‚æ•°: {calibrator.temperature:.4f}")
    print(f"  - æ ¡å‡†åæ¦‚ç‡èŒƒå›´: [{calibrated_probs.min():.4f}, {calibrated_probs.max():.4f}]")
    
    # è®¡ç®—æ ¡å‡†å‰åçš„ECE
    ece_before = metrics.expected_calibration_error(probs, targets)
    ece_after = metrics.expected_calibration_error(calibrated_probs, targets)
    improvement = (ece_before - ece_after) / ece_before * 100 if ece_before > 0 else 0
    
    print(f"  - ECEæ”¹è¿›: {improvement:.1f}% ({ece_before:.4f} â†’ {ece_after:.4f})")
except Exception as e:
    print(f"âœ— Temperature Scaling å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•5: éªŒè¯æ¡†æ¶
print("\nâœ… æµ‹è¯•5: å¤šç«™ç‚¹éªŒè¯æ¡†æ¶")
print("-" * 70)

try:
    from src.validation.cross_site_validator import CrossSiteValidator
    
    # ç”Ÿæˆæ¨¡æ‹Ÿå¤šç«™ç‚¹æ•°æ®
    sites_data = {}
    for site_idx in range(3):
        site_name = f"Site_{site_idx+1}"
        site_logits = np.random.randn(50, n_classes)
        exp_logits = np.exp(site_logits - site_logits.max(axis=1, keepdims=True))
        site_probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)
        site_targets = np.random.randint(0, n_classes, 50)
        sites_data[site_name] = {
            'probs': site_probs,
            'targets': site_targets
        }
    
    print(f"âœ“ ç”Ÿæˆ {len(sites_data)} ä¸ªç«™ç‚¹çš„æ¨¡æ‹Ÿæ•°æ®")
    
    # è®¡ç®—æ¯ä¸ªç«™ç‚¹çš„æŒ‡æ ‡
    for site, data in sites_data.items():
        site_ece = metrics.expected_calibration_error(data['probs'], data['targets'])
        print(f"  - {site}: ECE = {site_ece:.4f}")
        
except Exception as e:
    print(f"âœ— å¤šç«™ç‚¹éªŒè¯å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•6: æŠ¥å‘Šç”Ÿæˆ
print("\nâœ… æµ‹è¯•6: æŠ¥å‘Šç”Ÿæˆæ¡†æ¶")
print("-" * 70)

try:
    from src.validation.report_generator import ExternalValidationReportGenerator
    
    gen = ExternalValidationReportGenerator()
    print("âœ“ æŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
    print("âœ“ æ”¯æŒçš„æ–¹æ³•:")
    methods = [
        m for m in dir(gen) 
        if not m.startswith('_') and callable(getattr(gen, m))
    ]
    for method in methods[:5]:
        print(f"  - {method}")
    if len(methods) > 5:
        print(f"  ... å’Œ {len(methods)-5} ä¸ªå…¶ä»–æ–¹æ³•")
    
except Exception as e:
    print(f"âœ— æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# æ€»ç»“
print("\n" + "=" * 70)
print("âœ… æµ‹è¯•å®Œæˆï¼")
print("=" * 70)
print()
print("ğŸ“‹ åç»­æ­¥éª¤:")
print()
print("1ï¸âƒ£  å®‰è£…PyTorch:")
print("   conda install pytorch torchvision torchaudio -c pytorch")
print()
print("2ï¸âƒ£  ä¸‹è½½æ•°æ®é›†:")
print("   python data/DATASET_GUIDE.py")
print()
print("3ï¸âƒ£  è¿è¡Œå®Œæ•´éªŒè¯:")
print("   python scripts/cross_site_validation_example.py")
print()
