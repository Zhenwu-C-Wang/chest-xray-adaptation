
如果这个课题的目标是“顺便”写一篇**高质量论文**，那项目结构和实验规划就要从一开始就按“论文生产线”来设计，而不是只为了代码好用。

我帮你分成三个层次说：

1. **论文定位：你要讲的核心故事是什么**
2. **代码仓库结构：怎么为“可复现论文”服务**
3. **从仓库到论文：一条比较清晰的实验路线**

---

## 1. 先把论文“故事线”想清楚

围绕你最初的目标，可以把论文核心卖点定成类似：

> 在多公开胸片数据集上系统评估胸片分诊模型的**跨站点泛化**与**概率校准**，并展示简单可实现的**设备域自适应 + 校准 + 分层阈值**，在模拟临床场景中可显著降低危急片等待时间。

通常可以拆成几个 Results block / figure：

1. **Baseline 泛化性能**

   * 在“开发站点” vs “外部站点（NIH, CheXpert, MIMIC-CXR 等）”上的 AUROC / AUPRC
   * 分设备 / 分站点子群的性能稳定性

2. **校准分析**

   * 全局 ECE、分站点 / 分设备 ECE
   * 校准曲线（reliability diagrams），展示“从 miscalibrated → well-calibrated”

3. **域自适应 / 分层阈值的效果**

   * 无适配 vs 简单后处理（per-site threshold）vs 轻量 domain adaptation
   * 指标：AUROC 变化、ECE 变化、跨站点方差变化

4. **临床影响模拟**

   * 用 triage cutoff 模拟：危急片优先队列 vs 先到先读
   * 指标：危急片平均等待时间、95% 分位数、漏诊率

项目结构就要围绕这几个结果来反推：**每个 Figure / Table 对应一条可复现的 pipeline**。

---

## 2. 仓库结构：为“论文可复现”而设计

在原来那个通用结构基础上，可以再“论文导向化”一点，例如：

```text
chest-xray-adaptation/
│
├── README.md
├── LICENSE
├── requirements.txt / environment.yml
├── .gitignore
│
├── config/                     # 所有实验 & 论文结果用到的配置
│   ├── dataset/
│   │   ├── nih.yaml
│   │   ├── chexpert.yaml
│   │   └── mimic_cxr.yaml
│   ├── model/
│   │   ├── resnet50_baseline.yaml
│   │   └── resnet50_da.yaml
│   ├── exp/                    # 一行对应论文中的一个实验/图表
│   │   ├── exp1_internal_dev.yaml
│   │   ├── exp2_external_nih.yaml
│   │   ├── exp3_external_chexpert.yaml
│   │   ├── exp4_calibration.yaml
│   │   └── exp5_clinical_triage_sim.yaml
│   └── logging.yaml
│
├── data/
│   ├── metadata/               # 不放原始影像，只放索引/标签/设备信息
│   │   ├── nih_metadata.csv    # 包括: path, label, site_id, device_model, split
│   │   ├── chexpert_metadata.csv
│   │   └── mimic_metadata.csv
│   ├── splits/                 # 所有划分固定下来，便于复现
│   │   ├── nih_train_val_test.json
│   │   ├── chexpert_external.json
│   │   └── mimic_external.json
│   └── preprocessing/          # 预处理脚本（不一定放数据本身）
│       └── build_metadata.py
│
├── src/
│   ├── data/
│   │   ├── datasets.py         # Dataset / DataLoader, 支持按 site/device 分层
│   │   └── transforms.py       # 图像预处理、归一化、增强
│   ├── models/
│   │   ├── backbone_resnet.py
│   │   └── heads.py            # 多任务或多标签 head
│   ├── training/
│   │   ├── trainer.py
│   │   └── losses.py
│   ├── evaluation/
│   │   ├── metrics.py          # AUROC, AUPRC, ECE, Brier, group-wise metrics
│   │   └── eval_pipeline.py    # 一键跑完内外部验证
│   ├── calibration/
│   │   ├── temperature_scaling.py
│   │   ├── isotonic.py
│   │   └── reliability_diagram.py
│   ├── domain_adaptation/
│   │   ├── simple_da.py        # 简单 domain-specific BN / feature alignment
│   │   └── (可选) dann.py
│   ├── clinical/
│   │   └── triage_simulation.py # 根据阈值、预测分布模拟危急片等待时间
│   └── utils/
│       ├── seed.py
│       ├── config.py
│       └── logging.py
│
├── scripts/                    # 直接对应论文中的主要结果
│   ├── run_train_baseline.sh
│   ├── run_external_eval.sh
│   ├── run_calibration.sh
│   ├── run_domain_adaptation.sh
│   └── run_clinical_simulation.sh
│
├── experiments/                # 记录每次正式实验的配置、日志、结果
│   ├── exp1_internal_dev/
│   │   ├── config_used.yaml
│   │   ├── metrics.json
│   │   └── curves.pkl
│   ├── exp2_external_nih/
│   └── ...
│
├── notebooks/
│   ├── 01_eda_datasets.ipynb          # 数据分布 & 设备分层
│   ├── 02_fig_calibration_curves.ipynb
│   ├── 03_fig_site_wise_performance.ipynb
│   └── 04_fig_clinical_impact.ipynb
│
└── paper/
    ├── outline.md                     # 论文大纲/要点
    ├── figs/                          # 论文图的最终版本 (从 notebooks 导出)
    ├── tables/
    └── clinical_impact_one_pager.md
```

### 这个结构特别为“写论文”做的优化点：

* **config/exp/**：
  每个 `expX_*.yaml` 对应论文里的一个主要实验/结果。投稿时可以写：

  > All main experiments can be reproduced by running the scripts in `scripts/` with the configs in `config/exp/`.

* **experiments/**：
  真正跑出来的结果（metrics、日志、seed）都保存，并且跟 exp 编号对应，方便你后面改图、补表、不迷路。

* **notebooks/**：
  只负责“从已经算好的结果 → 出图/出表”。
  不在 notebook 里做训练/大规模运算，保证逻辑清晰、可复现。

* **paper/**：
  把 clinical impact one-pager、图、表都放这里，方便投稿 / 换期刊。

---

## 3. 从项目到论文：一条比较清晰的路线

你可以按下面的顺序推进，一边写代码一边“顺手写 paper”。

### Step 1：数据与站点/设备分层（Methods: Datasets）

* 在 `data/metadata/` 里：

  * 把每个样本的：`path, labels, site_id, device_model, acquisition_year, split` 固定下来
  * notebook `01_eda_datasets.ipynb` 出：

    * 各站点样本量
    * 各设备分布
    * 标签分布差异
      这些图和统计几乎都可以直接进论文。

### Step 2：Baseline 模型与内部验证（Methods: Model & Training）

* 用 `config/model/resnet50_baseline.yaml` + `exp1_internal_dev.yaml`，
* `scripts/run_train_baseline.sh` 训练，`experiments/exp1_internal_dev/` 固定结果。
* 记录：

  * 训练/验证曲线
  * 内部验证 AUROC / AUPRC / ECE
* 这些就是 “Baseline + Internal Validation” 的小节。

### Step 3：跨站外部验证（Results: Cross-site generalization）

* `scripts/run_external_eval.sh`，读取相同模型，对 NIH / CheXpert / MIMIC 分别评估：

  * Overall AUROC / AUPRC
  * **Per-site / per-device performance**
* 在 `notebooks/02_fig_site_wise_performance.ipynb` 输出：

  * 各站点 / 设备的箱线图 / 条形图
* 对应论文里一张“Across-site performance variability”的图。

### Step 4：校准分析（Results: Calibration）

* `scripts/run_calibration.sh`：

  * 在内部验证集上做温度缩放 / isotonic
  * 在外部站点上测试校准效果
* 输出：

  * 全局 ECE
  * 分站点 ECE
  * Reliability diagrams（多站点叠加或分图）
* 这部分可以强调：**“深度模型在跨站点时严重失校准，而简单后处理可以显著改善”**。

### Step 5：域自适应 / 分层阈值实验（Results: Domain adaptation）

* 在 `src/domain_adaptation/` 放一些**工程上可实现、论文上靠谱**的简单方法：

  * 例如：per-site BatchNorm 统计、feature alignment、fine-tuning with small labeled subset。
* 对比：

  * Baseline
  * * Calibration
  * * Domain adaptation
* 指标：

  * 平均 AUROC / ECE
  * 各站点性能的 **方差**（越小越好）
* 对应一张 ablation 图 + 一个表。

### Step 6：临床 triage 模拟（Results: Clinical impact）

* `src/clinical/triage_simulation.py`：

  * 输入：

    * 每张片的真实标签（是否危急）
    * 模型打分
    * 预设 triage 阈值 & 放射科医生处理速率
  * 输出：

    * 危急片平均等待时间
    * 95% / 99% 分位数等待时间
    * 漏掉的危急片比例（若你允许“低危直接降级处理”）
* `scripts/run_clinical_simulation.sh`：

  * baseline（FIFO）
  * triage（score-based priority）
  * triage + 校准 / + 域自适应 的对比
* `notebooks/04_fig_clinical_impact.ipynb`：

  * 出等待时间对比图、decision curve、或表格。
* 这就是你想要的“危急片等待时间 -20%”的那块结果。

### Step 7：写作与整理

* 在 `paper/outline.md` 写好大纲，结构可以是：

  1. Introduction
  2. Related Work
  3. Methods

     * Data & Sites
     * Model & Training
     * Domain Adaptation
     * Calibration
     * Clinical Simulation
  4. Results

     * Cross-site generalization
     * Calibration
     * Domain adaptation
     * Clinical impact
  5. Discussion
  6. Limitations & Future Work

* 同步维护 `paper/clinical_impact_one_pager.md`，可以作为：

  * 投稿 cover letter 的素材
  * 项目 README 中的简版介绍